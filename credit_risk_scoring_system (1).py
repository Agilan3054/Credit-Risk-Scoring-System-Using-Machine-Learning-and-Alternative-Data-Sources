# -*- coding: utf-8 -*-
"""Credit Risk Scoring System.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lnG8LgtzrwZYnjKNoowExgKwD3_H5VN5
"""

import requests
from bs4 import BeautifulSoup

def scrape_social_media_data(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')

    # Extract social media posts (you can customize this as needed)
    posts = soup.find_all('p', class_='post-text')
    social_media_data = [post.text for post in posts]

    return social_media_data

# Example usage
url = 'https://example-socialmedia.com/user/posts'
social_media_data = scrape_social_media_data(url)
print(social_media_data)

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, udf
from pyspark.ml.feature import VectorAssembler

# Initialize Spark session
spark = SparkSession.builder.appName('CreditRisk').getOrCreate()

# Load traditional and alternative datasets (financial + scraped data)
credit_data = spark.read.csv('credit_data.csv', header=True, inferSchema=True)
social_media_sentiment = spark.read.csv('social_media_sentiment.csv', header=True, inferSchema=True)

# Feature engineering: Transform raw data into meaningful features
assembler = VectorAssembler(inputCols=['income', 'debt', 'transaction_volume', 'sentiment_score'],
                            outputCol='features')
credit_data = assembler.transform(credit_data)

import nltk
from nltk.sentiment import SentimentIntensityAnalyzer

# Initialize sentiment analyzer
sia = SentimentIntensityAnalyzer()

def analyze_sentiment(text):
    score = sia.polarity_scores(text)
    return score['compound']

# Apply sentiment analysis to the scraped data
social_media_data['sentiment_score'] = social_media_data['post'].apply(analyze_sentiment)

# Example sentiment score output
print(social_media_data[['post', 'sentiment_score']])

import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.ensemble import RandomForestClassifier
from lightgbm import LGBMClassifier

# Load dataset and split into features and target
X = credit_data.select('features').toPandas()
y = credit_data.select('credit_risk_label').toPandas()

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# XGBoost Model
xgb_model = xgb.XGBClassifier()
xgb_model.fit(X_train, y_train)
xgb_preds = xgb_model.predict(X_test)
print("XGBoost Accuracy: ", accuracy_score(y_test, xgb_preds))

# Random Forest Model
rf_model = RandomForestClassifier()
rf_model.fit(X_train, y_train)
rf_preds = rf_model.predict(X_test)
print("Random Forest Accuracy: ", accuracy_score(y_test, rf_preds))

# LightGBM Model
lgbm_model = LGBMClassifier()
lgbm_model.fit(X_train, y_train)
lgbm_preds = lgbm_model.predict(X_test)
print("LightGBM Accuracy: ", accuracy_score(y_test, lgbm_preds))

# Confusion Matrix
print("Confusion Matrix:\n", confusion_matrix(y_test, lgbm_preds))

# Export model predictions to CSV for reporting
import pandas as pd

results_df = pd.DataFrame({
    'actual': y_test,
    'predicted': lgbm_preds
})
results_df.to_csv('credit_risk_predictions.csv', index=False)

